{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njfMcG71hJvf",
        "outputId": "ed73ef42-632f-4da9-e3f6-cf136847d246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "Bgs_jhXchN1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your Parquet file\n",
        "file_path = \"/content/drive/MyDrive/2nd sem/Big Data/data/train_meta.parquet\"\n",
        "\n",
        "# Open the Parquet file\n",
        "parquet_file = pq.ParquetFile(file_path)\n",
        "\n",
        "# Read the DataFrame normally (assuming 'event_id' is the index)\n",
        "filtered_meta = pd.read_parquet(file_path)\n"
      ],
      "metadata": {
        "id": "loAV-QJziXXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_meta = pd.read_csv('/content/drive/MyDrive/2nd sem/Big Data/data/filtered_meta.csv')"
      ],
      "metadata": {
        "id": "MLDLdXTS3MI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your Parquet file\n",
        "file_path = \"/content/drive/MyDrive/2nd sem/Big Data/data/batch_1.parquet\"\n",
        "\n",
        "# Open the Parquet file\n",
        "parquet_file = pq.ParquetFile(file_path)\n",
        "\n",
        "# Read the DataFrame normally (assuming 'event_id' is the index)\n",
        "df = pd.read_parquet(file_path)\n",
        "\n",
        "# Add a column with the index values (assuming the index is numerical)\n",
        "df['event_id'] = df.index  # Assuming the index is numerical\n",
        "\n",
        "# Drop the original index (if it's 'event_id')\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Print the first few rows of the dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZDGbFTgi0nL",
        "outputId": "20c96951-1859-495d-a04b-cb5a70956ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sensor_id  time  charge  auxiliary  event_id\n",
            "0       3918  5928   1.325       True        24\n",
            "1       4157  6115   1.175       True        24\n",
            "2       3520  6492   0.925       True        24\n",
            "3       5041  6665   0.225       True        24\n",
            "4       2948  8054   1.575       True        24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/2nd sem/Big Data/data/sensor_geometry.csv\"\n",
        "sens_df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "WPbAP5D_i-wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df, sens_df, on='sensor_id', how='inner')\n",
        "\n",
        "# Rename the x, y, and z columns from sens_df to avoid conflicts\n",
        "merged_df.sort_values(by=['event_id', 'time'], inplace=True)\n",
        "merged_df.reset_index(drop=True, inplace=True)\n",
        "# Create groups by event_id\n",
        "grouped_df = merged_df.groupby('event_id')\n",
        "\n",
        "# Calculate minimum time per event\n",
        "min_time_per_event = grouped_df['time'].transform('min')\n",
        "\n",
        "# Calculate time difference\n",
        "merged_df['time_diff'] = merged_df['time'] - min_time_per_event\n",
        "\n",
        "merged_df.sort_values(by=['event_id', 'time_diff'], inplace=True)\n",
        "\n",
        "merged_df.drop(['time'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "FJLmjiv0jDXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 100 rows from filtered_meta\n",
        "#filtered_meta_mark = filtered_meta.sample(12000)  # Randomly sample 100 rows\n",
        "filtered_meta_mark = filtered_meta.sample(3000)  # Use all rows\n",
        "\n",
        "\n",
        "# Define a function to filter merged_df based on event_id from a smaller filtered_meta subset\n",
        "def filter_merged_df_by_event_id(merged_df, filtered_meta_subset):\n",
        "\n",
        "  event_ids = filtered_meta_subset[\"event_id\"].tolist()  # Extract event_ids from the subset\n",
        "  return merged_df[merged_df[\"event_id\"].isin(event_ids) & (merged_df[\"auxiliary\"] == True)] # Filter based on event_ids\n",
        "\n",
        "# Filter merged_df based on sampled event_ids\n",
        "merged_df_mark = filter_merged_df_by_event_id(merged_df.copy(), filtered_meta_mark)  # Use a copy to avoid modifying original data\n",
        "merged_df_mark.drop('auxiliary', axis=1, inplace=True)\n",
        "merged_df_mark.reset_index(drop=True, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "xWOiBzAPjQaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_meta_mark.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtk_ulM53YgS",
        "outputId": "31d93de4-60eb-4bc4-df00-11608c0fa952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_3d_volumes(df, sens_df):\n",
        "    # Determine the maximum x, y, and z values from sens_df\n",
        "    max_x = (int(sens_df['x'].max()) - int(sens_df['x'].min()))//40 + 1\n",
        "    max_y = (int(sens_df['y'].max()) - int(sens_df['y'].min()))//40 + 1\n",
        "    max_z = (int(sens_df['z'].max()) - int(sens_df['z'].min()))//40 + 1\n",
        "\n",
        "    x_add = -int(sens_df['x'].min())\n",
        "    y_add = -int(sens_df['y'].min())\n",
        "    z_add = -int(sens_df['z'].min())\n",
        "\n",
        "\n",
        "    # Get unique event_ids\n",
        "    unique_event_ids = df['event_id'].unique()\n",
        "\n",
        "    # Initialize a dictionary to store volumes for each event_id\n",
        "    volumes = {}\n",
        "\n",
        "    # Iterate over unique event_ids\n",
        "    for event_id in unique_event_ids:\n",
        "        # Subset the DataFrame for the current event_id\n",
        "        event_df = df[df['event_id'] == event_id]\n",
        "\n",
        "\n",
        "        # Create an empty 3D volume filled with zeros\n",
        "        volume = np.zeros((max_x, max_y, max_z), dtype=np.float64)  # Adding 1 to include 0 values\n",
        "\n",
        "        # Fill the volume with time_diff values for each sensor_id\n",
        "        for index, row in event_df.iterrows():\n",
        "            x = (int(row['x']) + x_add)//40\n",
        "            y = (int(row['y']) + y_add)//40\n",
        "            z = (int(row['z']) + z_add)//40\n",
        "            volume[x, y, z] = row['time_diff']\n",
        "\n",
        "        # Store the volume in the dictionary\n",
        "        volumes[event_id] = volume\n",
        "\n",
        "    return volumes, unique_event_ids, max_x, max_y, max_z"
      ],
      "metadata": {
        "id": "jfuc_y83jZku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volumes, event_ids, max_x, max_y, max_z = create_3d_volumes(merged_df_mark, sens_df)"
      ],
      "metadata": {
        "id": "gI9xcKWzjcP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting DataFrame based on event_id\n",
        "filtered_meta_mark_sorted = filtered_meta_mark.set_index('event_id').reindex(event_ids).reset_index()\n",
        "\n",
        "# Extracting azimuth values\n",
        "azimuth_values = filtered_meta_mark_sorted['azimuth'].values\n",
        "zenith_values = filtered_meta_mark_sorted['zenith'].values"
      ],
      "metadata": {
        "id": "fY1iYs2cjfps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split your data into training and testing sets\n",
        "# Assuming volumes and azimuths are already defined\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(volumes.values()), azimuth_values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv3D(16, kernel_size=(3, 3, 3), activation='relu', input_shape=(max_x, max_y, max_z, 1)),\n",
        "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape training and testing data to fit the model input shape\n",
        "X_train = np.array(X_train).reshape(-1, max_x, max_y, max_z, 1)\n",
        "X_test = np.array(X_test).reshape(-1, max_x, max_y, max_z, 1)\n",
        "\n",
        "# Decrease batch size\n",
        "batch_size = 2\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=3,          # Number of epochs to wait before stopping\n",
        "    min_delta=0.0001,    # Minimum change in the monitored quantity\n",
        "    verbose=1            # Print messages\n",
        ")\n",
        "\n",
        "# Assuming you have defined your model somewhere above\n",
        "with tf.device('/gpu:0'):\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=40,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping_callback]  # Pass the EarlyStopping callback\n",
        "    )\n",
        "\n",
        "# Evaluate the trained model using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate root mean squared error (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "print(\"AZIMUTH\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)"
      ],
      "metadata": {
        "id": "PF3tzDHhjjZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382397a8-f35e-49d9-e7ab-5d34bbd210b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1080/1080 [==============================] - 5s 4ms/step - loss: 15217.0303 - val_loss: 22.4087\n",
            "Epoch 2/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 11960.9736 - val_loss: 12.4120\n",
            "Epoch 3/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 190.7859 - val_loss: 12.0388\n",
            "Epoch 4/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 12.1992 - val_loss: 11.4443\n",
            "Epoch 5/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 658.1434 - val_loss: 10.8933\n",
            "Epoch 6/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 51.8041 - val_loss: 9.7655\n",
            "Epoch 7/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 9.4015 - val_loss: 8.4142\n",
            "Epoch 8/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 7.8083 - val_loss: 6.8278\n",
            "Epoch 9/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 6.1625 - val_loss: 5.3392\n",
            "Epoch 10/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 4.7762 - val_loss: 4.2651\n",
            "Epoch 11/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 3.8841 - val_loss: 3.7038\n",
            "Epoch 12/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 3.4661 - val_loss: 3.5100\n",
            "Epoch 13/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 3.3247 - val_loss: 3.4658\n",
            "Epoch 14/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 3.2879 - val_loss: 3.4638\n",
            "Epoch 15/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 3.2786 - val_loss: 3.4693\n",
            "Epoch 16/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 3.2779 - val_loss: 3.4707\n",
            "Epoch 17/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 3.2775 - val_loss: 3.4726\n",
            "Epoch 17: early stopping\n",
            "19/19 [==============================] - 0s 4ms/step\n",
            "AZIMUTH\n",
            "RMSE: 1.7673143745043134\n",
            "MAE: 1.5268199025603153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split your data into training and testing sets\n",
        "# Assuming volumes and azimuths are already defined\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(volumes.values()), zenith_values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv3D(16, kernel_size=(3, 3, 3), activation='relu', input_shape=(max_x, max_y, max_z, 1)),\n",
        "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Reshape training and testing data to fit the model input shape\n",
        "X_train = np.array(X_train).reshape(-1, max_x, max_y, max_z, 1)\n",
        "X_test = np.array(X_test).reshape(-1, max_x, max_y, max_z, 1)\n",
        "\n",
        "# Decrease batch size\n",
        "batch_size = 2\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=3,          # Number of epochs to wait before stopping\n",
        "    min_delta=0.0001,    # Minimum change in the monitored quantity\n",
        "    verbose=1            # Print messages\n",
        ")\n",
        "\n",
        "# Assuming you have defined your model somewhere above\n",
        "with tf.device('/gpu:0'):\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=40,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping_callback]  # Pass the EarlyStopping callback\n",
        "    )\n",
        "\n",
        "# Evaluate the trained model using the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate root mean squared error (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "print(\"ZENITH\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nDwtBZBT9Sh",
        "outputId": "a498f28c-c71d-4110-fc39-aa7a088941c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1080/1080 [==============================] - 5s 4ms/step - loss: 18710.8066 - val_loss: 2.8877\n",
            "Epoch 2/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 26449.8574 - val_loss: 2.7430\n",
            "Epoch 3/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 6.3988 - val_loss: 2.6881\n",
            "Epoch 4/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 4.2772 - val_loss: 2.5374\n",
            "Epoch 5/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 821.6763 - val_loss: 2.3806\n",
            "Epoch 6/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 2.2690 - val_loss: 2.1532\n",
            "Epoch 7/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 1.9939 - val_loss: 1.8275\n",
            "Epoch 8/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 1.6230 - val_loss: 1.4157\n",
            "Epoch 9/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 1.1983 - val_loss: 0.9906\n",
            "Epoch 10/40\n",
            "1080/1080 [==============================] - 4s 3ms/step - loss: 0.8216 - val_loss: 0.6767\n",
            "Epoch 11/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5916 - val_loss: 0.5286\n",
            "Epoch 12/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 0.5092 - val_loss: 0.4932\n",
            "Epoch 13/40\n",
            "1080/1080 [==============================] - 4s 4ms/step - loss: 0.4952 - val_loss: 0.4898\n",
            "Epoch 14/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4945 - val_loss: 0.4897\n",
            "Epoch 15/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4946 - val_loss: 0.4898\n",
            "Epoch 16/40\n",
            "1080/1080 [==============================] - 3s 3ms/step - loss: 0.4945 - val_loss: 0.4898\n",
            "Epoch 16: early stopping\n",
            "19/19 [==============================] - 0s 5ms/step\n",
            "ZENITH\n",
            "RMSE: 0.7083075421690939\n",
            "MAE: 0.5843521935633311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDKh46FIVK2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}